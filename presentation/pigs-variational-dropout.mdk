[INCLUDE=presentation]
Title         : Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks


Reveal Theme  : solarized
Beamer Theme  : singapore

Bibliography  : pigs-variational-dropout-citations.bib
Bib style     : plainnat

[TITLE]

<script>
revealConfig.transition='linear';
</script>

# Contents 

[TOC]

# Turning Noise into Data

* You have a source distribution $S$ from which it is easy to sample.
* You have a target distribution $T$ known through samples $t_1, \dots, t_N$.
* Task is to learn a transformation $f_{\theta}: \mathcal{S} \to \mathcal{T}$ such that $f_{\theta}(S) \sim T$.


## Why is this a hard problem?


~Begin Vertical
## The Adversarial Solution

In "Generative Adversarial Networks" ([@gen_adv]) they propose a novel solution: 
~ Begin Framed
Train an adversary $D_{\phi}(x)$ to tell
the difference between `fake' generated samples and true data samples, and train the generator
$f_{\theta}(s)$ to fool the adversary.
~ End Framed

## Training the adversary
The adversary is trained to minmise the standard log-loss on the mixture distribution $M$ where true
and fake examples are equally likely
~Equation
L(\theta, \phi) = - \mathbb{E}_S \log D_{\phi}(S)  - \mathbb{E}_T \log \left (1-D_{\phi}(T) \right )
~
and assuming sufficient capacity, this is minimised in $\phi$ when
~Equation
D_{\phi}(x) = \frac{p_{\theta}(x)}{p_\theta(x) + p_T(x)}
~

##Training the generator
If we train the generator $f_{\theta}$ to maximise the loss of the adversary we get a maximin problem
~Equation
\max_{\theta} \min_{\phi} L(\theta, \phi)
~

~Equation
\min_{\theta} \mathbb{E}_S  \log \left (\frac{p_{\phi}(x)}{p_\phi(x) + p_T(x)} \right) + \mathbb{E}_T \log \left(1-\frac{p_{\phi}(x)}{p_\phi(x) + p_T(x)} \right)
~

~Equation
\min_{\theta}  D_{KL}(f_{\theta}(S) || M) + D_{KL}(M || f_{\theta}(S)) + C
~


~End Vertical

~Begin Vertical
## A Simple Example

[init_example]: images/init.png
![init_example]

We want to make the red samples look like the blue samples.

##Herding the Reds
<iframe width="700" height="400" src="https://www.youtube.com/embed/CILzNj2MP3s" frameborder="0" allowfullscreen></iframe>

~End Vertical

# A more general view

You learn a representation $H$ that is independent
of a binary variable $I$, this can be extended in various ways:

* Conditional Generative Models
* Domain Adaptation [@domain_adapt]
* Fairness
* ICA

# Iterative Approaches to Generating Images


* "DRAW"
* "Deep Generative Image Models Using A Laplacian Pyramid of Adversarial Networks"



~Begin Vertical
## Multiscale Decompositions of Images
A _Pyramid_ is a multiscale representation of an image or signal. 

##Gaussian Pyramid
We start with the initial image $I_0$ and
iteratively apply a blurring and downsampling function $d$ to give $I_{k+1} = d(I_k)$. The Gaussian Pyramid is
the sequence $[I_0, \dots, I_{n}]$.
[pyramid]: images/Image_pyramid.svg.png
![pyramid]

##Laplacian Pyramid
We want to represent the information that is unique to a particular scale. We define a function $u$
that does the opposite to $d$ - it upsamples and smoothes the image. Then define $h_k =I_k - u(I_{k+1})$ with 
$h_{n}=I_{n}$.
The sequence $[h_0, \dots, h_n]$ is called a _Laplacian Pyramid_.

[laplacian_pyramid]: images/laplacian_pyramid.jpg
![laplacian_pyramid]

~End Vertical

## Generative Models for the Laplacian Pyramid
* Build a generative model for $h_n = I_n$ using a standard generative adversarial network.
* For k < n, build a conditional generative network for $h_k | l_k$, where $l_k = u(I_{k+1})$.
* Each step is trained independently.

##Results

See [blog](http://soumith.ch/eyescream/).

##Other Generative Models
In NADE[@NADE] they use the generic chain rule
~Begin Equation 
p(x_1, \dots, x_k) = p(x_1 |x_2, \dots, x_k) \cdot p(x_2| x_3, \dots, x_k ) \cdot \, \dots \, \cdot p(x_k)
~End Equation
if you write 

~Begin Equation 
u(x_1, \dots, x_k) = (0, x_1, \dots, x_k)
~End Equation
 and 
 ~Begin Equation 
 d(x_1, \dots x_k) = (x_2, \dots, x_k),
 ~End Equation

## Bibliography {-}

[BIB]
